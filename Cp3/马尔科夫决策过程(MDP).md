# 马尔科夫决策过程(MDP)

##1.1 MDP介绍

MDP是马尔科夫链的一种扩展。提供了一个用于对决策情景建模的数学框架。   

MDP可以由5个关键要素表示：   

* 智能体能够真正处于的一组状态（S)。   
* 智能体从一种状态转移到另一种状态所执行的一组行为（A）。
* 转移概率（$P_{{ss}’}^{a}$）,这是执行某一个行为$a$,从一个状态$s$转移到另一个状态${s}'$的概率。
* 奖励概率（$R_{{ss}'}^a$），这是执行某一个行为$a$,状态转移后获得奖励的概率。
* 折扣因子($\gamma$),控制着即时奖励和未来奖励的重要性。   

   ### 1.1.1 奖励和回报

智能体试图使得从环境中获得的总奖励最大化，而不是及时奖励。智能体获得的奖励总额可计算如下：
$$
R_t=r_{t+1}+r_{t+2}+r_{t+3}+\cdots+r_T
$$

### 1.1.2 情景和连续任务

情景任务是具有一个终端状态的任务，在强化学习中，情景可以看做从初始状态到最终状态中智能体与环境的交互。

在连续任务中，**没有终端状态**。

### 1.1.3 折扣因数

已知智能体的任务是使得汇报最大化。对于一个情景任务，可以定义汇报为



